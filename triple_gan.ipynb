{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import cifar10\n",
    "from paddleops import *\n",
    "import time\n",
    "import paddle.fluid as fluid\n",
    "import paddle\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class triple_gan(object):\n",
    "    def __init__(self, epoch, batch_size, unlabel_batch_size, z_dim, dataset_name, n, gan_lr, cla_lr, result_dir, log_dir):\n",
    "        self.dataset_name = dataset_name\n",
    "        # self.checkpoint_dir = checkpoint_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.unlabelled_batch_size = unlabel_batch_size\n",
    "        self.test_batch_size = 1000\n",
    "        self.model_name = \"TripleGAN\"     # name for checkpoint\n",
    "        if self.dataset_name == 'cifar10' :\n",
    "            self.input_height = 32\n",
    "            self.input_width = 32\n",
    "            self.output_height = 32\n",
    "            self.output_width = 32\n",
    "\n",
    "            self.z_dim = z_dim\n",
    "            self.y_dim = 10\n",
    "            self.c_dim = 3\n",
    "\n",
    "\n",
    "            self.gan_lr = gan_lr\n",
    "            self.cla_lr = cla_lr\n",
    "            self.learning_rate = gan_lr # 3e-4, 1e-3\n",
    "            self.cla_learning_rate = cla_lr # 3e-3, 1e-2 ?\n",
    "            self.GAN_beta1 = 0.5\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.epsilon = 1e-8\n",
    "            self.alpha = 0.5\n",
    "            self.alpha_cla_adv = 0.01\n",
    "            self.init_alpha_p = 0.0 # 0.1, 0.03\n",
    "            self.apply_alpha_p = 0.1\n",
    "            self.alpha_p = 0.1 ##??\n",
    "\n",
    "            self.apply_epoch = 200 # 200, 300\n",
    "            self.decay_epoch = 50\n",
    "\n",
    "            self.sample_num = 64\n",
    "            self.visual_num = 100\n",
    "            self.len_discrete_code = 10\n",
    "\n",
    "            self.data_X, self.data_y, self.unlabelled_X, self.unlabelled_y, self.test_X, self.test_y = cifar10.prepare_data(n) # trainX, trainY, testX, testY\n",
    "\n",
    "            self.num_batches = len(self.data_X) // self.batch_size\n",
    "\n",
    "        else :\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def D(self, x, y_, name='Discriminator', is_test=False):\n",
    "        with fluid.unique_name.guard(name+'_'):\n",
    "            x = dropout(x, dropout_prob=0.2, is_test=False)\n",
    "            y = reshape(y_, [-1, 1, 1, self.y_dim]) #ten classes\n",
    "            x = conv_cond_concat(x, y)\n",
    "            #weight norm in paddlepaddle has finished\n",
    "            x = conv2d(x, num_filters=32, filter_size=[3,3], param_attr=wn('conv1'), name='conv1', act='lrelu')\n",
    "            x = conv_cond_concat(x, y)\n",
    "            x = conv2d(x, num_filters=32, filter_size=[3,3], stride=2, param_attr=wn('conv2'), name='conv2', act='lrelu')\n",
    "            x = dropout(x, dropout_prob=0.2)\n",
    "            x = conv_cond_concat(x, y)\n",
    "\n",
    "            x = conv2d(x, num_filters=64, filter_size=[3,3], param_attr=wn('conv3'), name='conv3', act='lrelu')\n",
    "            x = conv_cond_concat(x, y)\n",
    "            x = conv2d(x, num_filters=64, filter_size=[3,3], stride=2, param_attr=wn('conv4'), name='conv4', act='lrelu')\n",
    "            x = dropout(x, dropout_prob=0.2)\n",
    "            x = conv_cond_concat(x, y)\n",
    "\n",
    "            x = conv2d(x, num_filters=128, filter_size=[3,3], param_attr=wn('conv5'), name='conv5', act='lrelu')\n",
    "            x = conv_cond_concat(x, y)\n",
    "            x = conv2d(x, num_filters=128, filter_size=[3,3], param_attr=wn('conv6'), name='conv6', act='lrelu')\n",
    "            x = conv_cond_concat(x, y)\n",
    "            \n",
    "            x = Global_Average_Pooling(x)\n",
    "            x = flatten(x)\n",
    "            x = concat(x, y_)\n",
    "            #IcGAN 每一层都要concat一下\n",
    "\n",
    "            # MLP??\n",
    "            x_logit = fc(x, 1, name='fc')\n",
    "            out = sigmoid(x_logit, name='sigmoid')\n",
    "\n",
    "            return out, x_logit, x\n",
    "\n",
    "\n",
    "    def G(self, z, y, name='Generator', is_test=False):\n",
    "        with fluid.unique_name.guard(name+'_'):\n",
    "            zy=concat(z,y)\n",
    "            zy = fc(zy, 8192, name='fc', act='relu')\n",
    "            zy = bn(zy, name='bn', act='relu')\n",
    "            zy = reshape(zy, [-1, 4, 4, 512])\n",
    "            y = reshape(y, [-1, 1, 1, self.y_dim])\n",
    "            zy = conv_cond_concat(zy, y)\n",
    "            zy = deconv(zy, num_filters=256, filter_size=[5, 5], stride=2, name='deconv1')\n",
    "            zy = bn(zy, act='relu')\n",
    "\n",
    "            zy = conv_cond_concat(zy, y)\n",
    "            zy = deconv(zy, num_filters=128, filter_size=[5, 5], stride= 2, name='deconv2')\n",
    "            zy = bn(zy, act='relu')\n",
    "\n",
    "            zy = conv_cond_concat(zy, y)\n",
    "            zy = deconv(zy, num_filters=3, filter_size=[5, 5], stride=2, \n",
    "                            param_attr=wn(name='deconv3'), name='deconv3', act='tanh')\n",
    "            \n",
    "            return zy\n",
    "\n",
    "    def C(self, x, name='Classifier', is_test=False):\n",
    "        x = gaussian_noise_layer(x, std=0.15)\n",
    "        x = conv2d(x, num_filters=128, filter_size=[3,3], act='lrelu', param_attr=wn('conv1'), name='conv1')\n",
    "        x = conv2d(x, num_filters=128, filter_size=[3,3], act='lrelu', param_attr=wn('conv2'), name='conv2')\n",
    "        x = conv2d(x, num_filters=128, filter_size=[3,3], act='lrelu', param_attr=wn('conv3'), name='conv3')\n",
    "        x = max_pooling(x , pool_size=[2,2])\n",
    "        x = dropout(x, dropout_prob=0.5)\n",
    "\n",
    "        x = conv2d(x, num_filters=256, filter_size=[3,3], act='lrelu', param_attr=wn('conv4'), name='conv4')\n",
    "        x = conv2d(x, num_filters=256, filter_size=[3,3], act='lrelu', param_attr=wn('conv5'), name='conv5')\n",
    "        x = conv2d(x, num_filters=256, filter_size=[3,3], act='lrelu', param_attr=wn('conv5'), name='conv5')\n",
    "        x = max_pooling(x , pool_size=[2,2])\n",
    "        x = dropout(x, dropout_prob=0.5)\n",
    "\n",
    "        x = conv2d(x, num_filters=512, filter_size=[3,3], act='lrelu', param_attr=wn('conv5'), name='conv5')\n",
    "        x = nin(x, 256, param_attr=wn('nin1'), act='lrelu')\n",
    "        x = nin(x, 128, param_attr=wn('nin1'), act='lrelu')\n",
    "        x = Global_Average_Pooling(x)\n",
    "        x = flatten(x)\n",
    "        x = fc(x, 10, param_attr=wn('fc1'), name=name+'_fc1')\n",
    "        out = softmax(x)\n",
    "\n",
    "        return x, out\n",
    "\n",
    "    def loss(self, x, label):\n",
    "        return fluid.layers.mean(\n",
    "            fluid.layers.sigmoid_cross_entropy_with_logits(\n",
    "                x=x, label=label))\n",
    "    \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}_{}\".format(\n",
    "            self.model_name, self.dataset_name,\n",
    "            self.batch_size, self.z_dim)\n",
    "\n",
    "    def train(self):\n",
    "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "        bs = self.batch_size\n",
    "        unlabel_bs = self.unlabelled_batch_size\n",
    "        test_bs = self.test_batch_size\n",
    "        alpha = self.alpha\n",
    "        alpha_cla_adv = self.alpha_cla_adv\n",
    "\n",
    "        def declare_data(self):\n",
    "            # images\n",
    "            self.inputs = fluid.layers.data(shape=[bs] + image_dims, name='real_images')\n",
    "            self.unlabelled_inputs = fluid.layers.data(shape=[unlabel_bs] + image_dims, name='unlabelled_images')\n",
    "            self.test_inputs = fluid.layers.data(shape=[test_bs] + image_dims, name='test_images')\n",
    "\n",
    "            # labels\n",
    "            self.y = fluid.layers.data(shape=[bs, self.y_dim], name='y')\n",
    "            self.unlabelled_inputs_y = fluid.layers.data(shape=[unlabel_bs, self.y_dim], name='unlabelled_images_y')\n",
    "            self.test_label = fluid.layers.data(shape=[test_bs, self.y_dim], name='test_label')\n",
    "            self.visual_y = fluid.layers.data(shape=[self.visual_num, self.y_dim], name='visual_y')\n",
    "\n",
    "            # noises\n",
    "            self.z = fluid.layers.data(shape=[bs, self.z_dim], name='z')\n",
    "            self.visual_z = fluid.layers.data(shape=[self.visual_num, self.z_dim], name='visual_z')\n",
    "\n",
    "\n",
    "        d_program = fluid.Program()\n",
    "        g_program = fluid.Program()\n",
    "        c_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(d_program):\n",
    "            declare_data(self)\n",
    "            D_real, D_real_logits, _ = self.D(self.inputs, self.y, is_test=False)\n",
    "            G_train = self.G(self.z, self.y, is_test=False)\n",
    "            D_fake, D_fake_logits, _ = self.D(G_train, self.y, is_test=False)\n",
    "            D_cla, D_cla_logits = self.C(self.unlabelled_inputs, is_test=False)\n",
    "\n",
    "\n",
    "            # ones_real = fluid.layers.fill_constant_batch_size_like(D_real, shape=[-1, 1], dtype='float32', value=1)\n",
    "            # zeros_fake = fluid.layers.fill_constant_batch_size_like(D_fake, shape=[-1, 1], dtype='float32', value=1)\n",
    "            # zeros_cla = fluid.layers.fill_constant_batch_size_like(D_cla, shape=[-1, 1], dtype='float32', value=1)\n",
    "            ones_real = ones(D_real.shape)\n",
    "            zeros_fake = zeros(D_fake.shape)\n",
    "            zeros_cla = zeros(D_cla.shape)\n",
    "\n",
    "            ce_real = fluid.layers.sigmoid_cross_entropy_with_logits(x=D_real_logits, label=ones_real)\n",
    "            ce_fake = fluid.layers.sigmoid_cross_entropy_with_logits(x=D_fake_logits, label=zeros_fake)\n",
    "            ce_cla = fluid.layers.sigmoid_cross_entropy_with_logits(x=D_cla_logits, label=zeros_cla)\n",
    "\n",
    "            d_loss_real = fluid.layers.reduce_mean(ce_real)\n",
    "            d_loss_fake = (1 - alpha) * fluid.layers.reduce_mean(ce_fake)\n",
    "            d_loss_cla = alpha * fluid.layers.reduce_mean(ce_cla)\n",
    "            self.d_loss = d_loss_real + d_loss_fake + d_loss_cla\n",
    "\n",
    "        with fluid.program_guard(c_program):\n",
    "            declare_data(self)\n",
    "            G_train = self.G(self.z, self.y, is_test=False)\n",
    "            # D_fake, D_fake_logits, _ = self.D(G_train, self.y)\n",
    "\n",
    "            # ones_fake = fluid.layers.fill_constant_batch_size_like(D_fake, shape=[-1, 1], dtype='float32', value=1)\n",
    "            # ce_fake_g = fluid.layers.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=ones_fake)\n",
    "            # self.g_loss = (1 - alpha) * fluid.layers.reduce_mean(ce_fake_g)\n",
    "            \n",
    "            C_real_logits = self.C(self.inputs, is_test=False)\n",
    "            R_L = fluid.layers.reduce_mean(fluid.layers.softmax_with_cross_entropy(label=self.y, logits=C_real_logits))\n",
    "\n",
    "            # output of D for unlabelled imagesc\n",
    "            Y_c = self.C(self.unlabelled_inputs, is_test=False)\n",
    "            D_cla, D_cla_logits, _ = self.D(self.unlabelled_inputs, Y_c, is_test=False)\n",
    "            ones_D_cla = fluid.layers.fill_constant_batch_size_like(D_cla, shape=[-1, 1], dtype='float32', value=1)\n",
    "\n",
    "\n",
    "            # output of C for fake images\n",
    "            C_fake_logits = self.C(G_train, is_test=False)\n",
    "            R_P = fluid.layers.reduce_mean(fluid.layers.softmax_with_cross_entropy(label=self.y, logits=C_fake_logits))\n",
    "\n",
    "            max_c = fluid.layers.cast(fluid.layers.argmax(Y_c, axis=1), dtype='float32')\n",
    "            c_loss_dis = fluid.layers.reduce_mean(max_c * fluid.layers.softmax_with_cross_entropy(logits=D_cla_logits, label=ones_D_cla))\n",
    "            # self.c_loss = alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
    "\n",
    "            # R_UL = self.unsup_weight * tf.reduce_mean(tf.squared_difference(Y_c, self.unlabelled_inputs_y))\n",
    "            self.c_loss = alpha_cla_adv * alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
    "\n",
    "\n",
    "        with fluid.program_guard(g_program):\n",
    "            declare_data(self)\n",
    "            G_train = self.G(self.z, self.y, is_test=False)\n",
    "            self.infer_program = g_program.clone(for_test=True)\n",
    "            D_fake, D_fake_logits, _ = self.D(G_train, self.y, is_test=False)\n",
    "\n",
    "            # ones_fake = fluid.layers.fill_constant_batch_size_like(D_fake, shape=[-1, 1], dtype='float32', value=1)\n",
    "            ones_fake = ones(D_fake.shape)\n",
    "            ce_fake_g = fluid.layers.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=ones_fake)\n",
    "            self.g_loss = (1 - alpha) * fluid.layers.reduce_mean(ce_fake_g)\n",
    "\n",
    "\n",
    "        fluid.optimizer.Adam(self.gan_lr, beta1=self.GAN_beta1).minimize(loss=self.d_loss)\n",
    "\n",
    "        c_parameters = [p.name for p in c_program.global_block().all_parameters()]\n",
    "        fluid.optimizer.Adam(self.gan_lr, beta1=self.GAN_beta1).minimize(loss=self.c_loss, parameter_list=c_parameters)\n",
    "\n",
    "        g_parameters = [p.name for p in g_program.global_block().all_parameters()]\n",
    "        fluid.optimizer.Adam(self.cla_lr, beta1=self.beta1, beta2=self.beta2, epsilon=self.epsilon).minimize(loss=self.g_loss, parameter_list=g_parameters)\n",
    "\n",
    "        place = fluid.CUDAPlace(0) if fluid.core.is_compiled_with_cuda() else fluid.CPUPlace()\n",
    "        self.exe = fluid.Executor(place)\n",
    "        self.exe.run(fluid.default_startup_program())\n",
    "\n",
    "        start_epoch = 0\n",
    "\n",
    "        gan_lr = self.learning_rate\n",
    "        cla_lr = self.cla_learning_rate\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.sample_z = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
    "        self.test_codes = self.data_y[0:self.visual_num]\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "            if epoch >= self.decay_epoch :\n",
    "                gan_lr *= 0.995\n",
    "                cla_lr *= 0.99\n",
    "                print(\"**** learning rate DECAY ****\")\n",
    "                print(gan_lr)\n",
    "                print(cla_lr)\n",
    "\n",
    "            if epoch >= self.apply_epoch :\n",
    "                alpha_p = self.apply_alpha_p\n",
    "            else :\n",
    "                alpha_p = self.init_alpha_p\n",
    "\n",
    "            # rampup_value = rampup(epoch - 1)\n",
    "            # unsup_weight = rampup_value * 100.0 if epoch > 1 else 0\n",
    "\n",
    "            \n",
    "            epoch_d_loss = []\n",
    "            epoch_c_loss = []\n",
    "            epoch_g_loss = []\n",
    "\n",
    "        # get batch data\n",
    "            for idx in range(0, self.num_batches):\n",
    "                batch_images = self.data_X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "                batch_codes = self.data_y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "                batch_unlabelled_images = self.unlabelled_X[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
    "                batch_unlabelled_images_y = self.unlabelled_y[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
    "\n",
    "                batch_z = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.inputs: batch_images, self.y: batch_codes,\n",
    "                    self.unlabelled_inputs: batch_unlabelled_images,\n",
    "                    self.unlabelled_inputs_y: batch_unlabelled_images_y,\n",
    "                    self.z: batch_z, self.alpha_p: alpha_p,\n",
    "                    self.gan_lr: gan_lr, self.cla_lr: cla_lr\n",
    "                    # self.unsup_weight : unsup_weight\n",
    "                }\n",
    "                # update D network\n",
    "                # _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss], feed_dict=feed_dict)\n",
    "                # self.writer.add_summary(summary_str, counter)\n",
    "                d_loss = self.exe.run(d_program, feed=feed_dict, fetch_list={self.d_loss})\n",
    "                g_loss = self.exe.run(g_program, feed=feed_dict, fetch_list={self.g_loss})\n",
    "                c_loss = self.exe.run(c_program, feed=feed_dict, fetch_list={self.c_loss})\n",
    "\n",
    "\n",
    "                # # update G network\n",
    "                # _, summary_str_g, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], feed_dict=feed_dict)\n",
    "                # self.writer.add_summary(summary_str_g, counter)\n",
    "\n",
    "                # # update C network\n",
    "                # _, summary_str_c, c_loss = self.sess.run([self.c_optim, self.c_sum, self.c_loss], feed_dict=feed_dict)\n",
    "                # self.writer.add_summary(summary_str_c, counter)\n",
    "\n",
    "                # display training status\n",
    "                # counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, c_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss, c_loss))\n",
    "\n",
    "                # save training results for every 100 steps\n",
    "                \"\"\"\n",
    "                if np.mod(counter, 100) == 0:\n",
    "                    samples = self.sess.run(self.infer_program,\n",
    "                                            feed_dict={self.z: self.sample_z, self.y: self.test_codes})\n",
    "                    image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
    "                    save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                                './' + check_folder(\n",
    "                                    self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_train_{:02d}_{:04d}.png'.format(\n",
    "                                    epoch, idx))\n",
    "                \"\"\"\n",
    "\n",
    "            # classifier test\n",
    "            # test_acc = 0.0\n",
    "\n",
    "            # for idx in range(10) :\n",
    "            #     test_batch_x = self.test_X[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
    "            #     test_batch_y = self.test_y[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
    "\n",
    "            #     acc_ = self.sess.run(self.accuracy, feed_dict={\n",
    "            #         self.test_inputs: test_batch_x,\n",
    "            #         self.test_label: test_batch_y\n",
    "            #     })\n",
    "\n",
    "            #     test_acc += acc_\n",
    "            # test_acc /= 10\n",
    "\n",
    "            # summary_test = tf.Summary(value=[tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
    "            # self.writer.add_summary(summary_test, epoch)\n",
    "\n",
    "            # line = \"Epoch: [%2d], test_acc: %.4f\\n\" % (epoch, test_acc)\n",
    "            # print(line)\n",
    "            # lr = \"{} {}\".format(gan_lr, cla_lr)\n",
    "            # with open('logs.txt', 'a') as f:\n",
    "            #     f.write(line)\n",
    "            # with open('lr_logs.txt', 'a') as f :\n",
    "            #     f.write(lr+'\\n')\n",
    "\n",
    "            # After an epoch, start_batch_id is set to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model\n",
    "            # self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "            # show temporal results\n",
    "            self.visualize_results(epoch)\n",
    "\n",
    "            # save model for final step\n",
    "        # self.save(self.checkpoint_dir, counter)\n",
    "    def visualize_results(self, epoch):\n",
    "        # tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
    "        z_sample = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
    "\n",
    "        \"\"\" random noise, random discrete code, fixed continuous code \"\"\"\n",
    "        y = np.random.choice(self.len_discrete_code, self.visual_num)\n",
    "        # Generated 10 labels with batch_size\n",
    "        y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
    "        y_one_hot[np.arange(self.visual_num), y] = 1\n",
    "\n",
    "        samples = self.exe.run(self.infer_program, feed={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
    "\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                    check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir + '/all_classes') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
    "\n",
    "        \"\"\" specified condition, random noise \"\"\"\n",
    "        n_styles = 10  # must be less than or equal to self.batch_size\n",
    "\n",
    "        np.random.seed()\n",
    "        si = np.random.choice(self.visual_num, n_styles)\n",
    "\n",
    "        for l in range(self.len_discrete_code):\n",
    "            y = np.zeros(self.visual_num, dtype=np.int64) + l\n",
    "            y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
    "            y_one_hot[np.arange(self.visual_num), y] = 1\n",
    "\n",
    "            samples = self.exe.run(self.infer_program, feed={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
    "            save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                        check_folder(\n",
    "                            self.result_dir + '/' + self.model_dir + '/class_%d' % l) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_class_%d.png' % l)\n",
    "\n",
    "            samples = samples[si, :, :, :]\n",
    "\n",
    "            if l == 0:\n",
    "                all_samples = samples\n",
    "            else:\n",
    "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
    "\n",
    "        \"\"\" save merged images to check style-consistency \"\"\"\n",
    "        canvas = np.zeros_like(all_samples)\n",
    "        for s in range(n_styles):\n",
    "            for c in range(self.len_discrete_code):\n",
    "                canvas[s * self.len_discrete_code + c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
    "\n",
    "        save_images(canvas, [n_styles, self.len_discrete_code],\n",
    "                    check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir + '/all_classes_style_by_style') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes_style_by_style.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('labelled data:', (4000, 32, 32, 3), (4000, 1))\n",
      "('unlabelled data :', (50000, 32, 32, 3), (50000, 1))\n",
      "('Test data :', (10000, 32, 32, 3), (10000, 1))\n",
      "======Load finished======\n",
      "======Shuffling data======\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-be0cce3b8a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./log/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcheck_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mGAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriple_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabel_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcla_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3bafb10278b5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, epoch, batch_size, unlabel_batch_size, z_dim, dataset_name, n, gan_lr, cla_lr, result_dir, log_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_discrete_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlabelled_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlabelled_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# trainX, trainY, testX, testY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/todd/study/reimplement/triple-gan/TripleGAN-Tensorflow/cifar10.pyc\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0munlabelled_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munlabelled_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0munlabelled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munlabelled_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    epoch = 1000\n",
    "    batch_size = 20\n",
    "    unlabel_batch_size = 250\n",
    "    z_dim = 100\n",
    "    dataset_name = 0\n",
    "    n = 4000\n",
    "    gan_lr = 3e-4\n",
    "    cla_lr = 3e-3\n",
    "    # checkpoint_dir = 0\n",
    "    dataset_name = 'cifar10'\n",
    "    result_dir = './result/'\n",
    "    check_folder(result_dir)\n",
    "    log_dir = './log/'\n",
    "    check_folder(log_dir)\n",
    "    GAN = triple_gan(epoch, batch_size, unlabel_batch_size, z_dim, dataset_name, n, gan_lr, cla_lr, result_dir, log_dir)\n",
    "    GAN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
